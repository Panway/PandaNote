# å‰è¨€

åŸæ–‡åœ°å€ [blog.beezwax.net](https://blog.beezwax.net/2017/07/07/writing-a-markdown-compiler/)

> æœ¯è¯­åŠç¿»è¯‘ï¼š
>
> Tokenï¼šæ ‡è®°
>
> Tokenizingï¼šæ ‡è®°åŒ–
>
> Abstract Syntax Tree, or ASTï¼šæŠ½è±¡è¯­æ³•æ ‘
>
> terminalï¼šç»ˆç»“ç¬¦
>
> Backus Normal Formï¼ŒBNFï¼šå·´ç§‘æ–¯èŒƒå¼ï¼Œåˆç§°ä¸ºå·´ç§‘æ–¯-è¯ºå°”èŒƒå¼ï¼Œæ˜¯ä¸€ç§ç”¨äºè¡¨ç¤ºä¸Šä¸‹æ–‡æ— å…³æ–‡æ³•çš„è¯­è¨€ï¼Œä¸Šä¸‹æ–‡æ— å…³æ–‡æ³•æè¿°äº†ä¸€ç±»å½¢å¼è¯­è¨€ã€‚å®ƒæ˜¯ç”±çº¦ç¿°Â·å·´ç§‘æ–¯ï¼ˆJohn Backusï¼‰å’Œå½¼å¾—Â·è¯ºå°”ï¼ˆPeter Naurï¼‰é¦–å…ˆå¼•å…¥çš„ç”¨æ¥æè¿°è®¡ç®—æœºè¯­è¨€è¯­æ³•çš„ç¬¦å·é›†ã€‚

# Part1

Have you ever wanted to make your own programming language? Maybe a template engine? A JSON parser? If you have ever built any of those, you might have noticed itâ€™s not exactly easy to get started. There are a lot of concepts to digest before you get going. Thatâ€™s why lots of devs just give up. Weâ€™d like to help with that.
ä½ æƒ³è¿‡åˆ›å»ºè‡ªå·±çš„ç¼–ç¨‹è¯­è¨€å—ï¼Ÿæˆ–è€…ä¸€ä¸ªæ¨¡æ¿å¼•æ“ã€ä¸€ä¸ª JSON è§£æå™¨ï¼Ÿå¦‚æœä½ æ„å»ºè¿‡å…¶ä¸­çš„ä»»ä½•ä¸€ä¸ªï¼Œä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°å®ƒä¸æ˜¯é‚£ä¹ˆå®¹æ˜“å¼€å§‹ã€‚åœ¨å¼€å§‹ä¹‹å‰ï¼Œæœ‰å¾ˆå¤šæ¦‚å¿µéœ€è¦ç†è§£ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå¾ˆå¤šå¼€å‘è€…ä¼šæ”¾å¼ƒã€‚ç°åœ¨æˆ‘ä»¬å¾ˆä¹æ„å¸®å¸®ä½ ã€‚

At Beezwax, a few years ago we built [a WordPress plugin](https://github.com/beezwax/WP-Publish-to-Apple-News) which allows users to upload their blog posts to [the Apple News platform](https://www.apple.com/news/). In order to do this, we had to translate HTML to some particular format. What we wrote is, at its core, a compiler. Compilers are not only for programming languages, they are in many more places than you might think!
åœ¨ Beezwaxï¼Œå‡ å¹´å‰æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ª WordPress æ’ä»¶ï¼Œå…è®¸ç”¨æˆ·å°†ä»–ä»¬çš„åšå®¢æ–‡ç« ä¸Šä¼ åˆ° Apple News å¹³å°ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¿…é¡»å°† HTML ç¿»è¯‘æˆæŸç§ç‰¹å®šçš„æ ¼å¼ã€‚æˆ‘ä»¬å†™çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªç¼–è¯‘å™¨ï¼Œç¼–è¯‘å™¨ä¸ä»…é€‚ç”¨äºç¼–ç¨‹è¯­è¨€ï¼Œå®ƒä»¬çš„ä½¿ç”¨åœºæ™¯æ¯”ä½ æƒ³è±¡çš„è¦å¤šå¾—å¤šï¼

This series of blog posts will show you how to make a compiler from scratch. The techniques displayed here will not only help you write compilers, but will give you the tools to solve a whole type of similar problems which â€“ in the programming world â€“ happen quite frequently.

æœ¬ç³»åˆ—åšå®¢æ–‡ç« å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä»å¤´å¼€å§‹åˆ¶ä½œä¸€ä¸ªç¼–è¯‘å™¨ã€‚è¿™é‡Œå±•ç¤ºçš„æŠ€æœ¯ä¸ä»…å¯ä»¥å¸®åŠ©æ‚¨ç¼–å†™ç¼–è¯‘å™¨ï¼Œè€Œä¸”è¿˜å¯ä»¥ä¸ºæ‚¨æä¾›å·¥å…·æ¥è§£å†³åœ¨ç¼–ç¨‹ä¸–ç•Œä¸­ç»å¸¸å‘ç”Ÿçš„ç±»ä¼¼é—®é¢˜ã€‚

What exactly is a compiler, anyways? ç¼–è¯‘å™¨åˆ°åº•æ˜¯ä»€ä¹ˆ
------------------------------------

Letâ€™s start from the beginning and define what a compiler is. A compiler is just a black box which translates input in a given language to output in another language. The input and output languages can be anything. If youâ€™ve been in the Javascript world for the past few years you might have seen something called _transpiler_. A transpiler is actually a compiler, it transforms, for example, _Coffeescript_ source code into _Javascript_ source code or _SASS_ into _CSS_.

ç¼–è¯‘å™¨å°±æ˜¯ä¸€ä¸ªé»‘ç›’å­ï¼Œå®ƒå°†ç»™å®šçš„ä¸€ç§è¯­è¨€è¾“å…¥ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€è¾“å‡ºã€‚è¾“å…¥å’Œè¾“å‡ºè¯­è¨€å¯ä»¥æ˜¯ä»»ä½•è¯­è¨€ã€‚å¦‚æœä½ å·²ç»åœ¨ Javascript ä¸–ç•Œå‘†äº†å‡ å¹´ï¼Œä½ å¯èƒ½ä¼šçœ‹åˆ°ä¸€äº›å«åš _transpiler_ çš„ä¸œè¥¿ã€‚Transpiler å®é™…ä¸Šæ˜¯ä¸€ä¸ªç¼–è¯‘å™¨ï¼Œä¾‹å¦‚å®ƒæŠŠ Coffeescript æºä»£ç è½¬æ¢æˆ Javascript æºä»£ç æˆ–è€… SASS è½¬æ¢æˆ CSSã€‚

> **NOTE** Compilers canâ€™t take any language as input. With these techniques, you cannot write an english-to-machine-code compiler. But for _simple_ languages, we can. Once we get into parsing weâ€™ll learn more about those kind of languages, for now, just know that every programming language you know can be an input language for a compiler.
>
> **æ³¨æ„**ï¼šç¼–è¯‘å™¨ä¸èƒ½å°†ä»»ä½•è¯­è¨€ä½œä¸ºè¾“å…¥ã€‚æ‚¨æ— æ³•å†™ä¸€ä¸ªè‹±è¯­åˆ°æœºå™¨ä»£ç çš„ç¼–è¯‘å™¨ã€‚ä½†æ˜¯å¯¹äº*ç®€å•çš„*è¯­è¨€ï¼Œæˆ‘ä»¬å¯ä»¥ã€‚ä¸€æ—¦æˆ‘ä»¬å¼€å§‹è§£æï¼Œæˆ‘ä»¬å°†äº†è§£æ›´å¤šå…³äºè¿™äº›ç±»å‹çš„è¯­è¨€ï¼Œç°åœ¨ï¼Œä½ åªéœ€çŸ¥é“æ¯ä¸€ç§ç¼–ç¨‹è¯­è¨€éƒ½å¯ä»¥ä½œä¸ºç¼–è¯‘å™¨çš„è¾“å…¥è¯­è¨€ã€‚

What weâ€™ll build æˆ‘ä»¬å°†æ„å»ºä»€ä¹ˆ
----------------

To keep things simple, I decided to make a simple compiler which translates a tiny subset of markdown to HTML. Hereâ€™s an example:

ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘å†³å®šå†™ä¸€ä¸ªç®€å•çš„ç¼–è¯‘å™¨ï¼Œå°† Markdown çš„ä¸€ä¸ªå°å­é›†ç¿»è¯‘æˆ HTMLã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªä¾‹å­ï¼š

```ruby
Markdown.to_html('_Foo_ **bar**') # => "<p><em>Foo<em> <strong>bar<strong></p>"
```

As you can see, we put markdown in, and get back HTML. For the implementation language, Iâ€™ve chosen Ruby, a language we love at Beezwax because of its focus on readability and programmer happiness. As I want to focus on concepts rather than a fully-optimized implementation, I think Ruby is the best fit for these tutorials.

å¦‚æ‚¨æ‰€è§ï¼Œæˆ‘ä»¬æ”¾å…¥ Markdownï¼Œè¿”å› HTMLã€‚å®ç°è¯­è¨€æˆ‘é€‰æ‹©äº† Rubyï¼Œå› ä¸ºå®ƒçš„å¯è¯»æ€§æ›´å¥½ã€‚å› ä¸ºæˆ‘æƒ³ä¸“æ³¨äºæ¦‚å¿µè€Œä¸æ˜¯å®Œå…¨ä¼˜åŒ–çš„å®ç°ï¼Œæ‰€ä»¥æˆ‘è®¤ä¸º Ruby æœ€é€‚åˆè¿™äº›æ•™ç¨‹ã€‚

Youâ€™ll learn about tokenization, parsing, and code-generation. Because Iâ€™ll talk about compilers, I wonâ€™t get into things like interpreters or optimizations. I just want to give the reader a solid base, so they can get a taste of this whole subject, and pursue their own more specific interests if they happen to like it.

æ‚¨å°†äº†è§£æ ‡è®°åŒ–ï¼ˆtokenizationï¼‰ã€è§£æå’Œä»£ç ç”Ÿæˆã€‚å› ä¸ºæˆ‘å°†è®¨è®ºç¼–è¯‘å™¨ï¼Œæ‰€ä»¥æˆ‘ä¸ä¼šè®¨è®ºè§£é‡Šå™¨ï¼ˆinterpretersï¼‰æˆ–ä¼˜åŒ–ä¹‹ç±»çš„ä¸œè¥¿ã€‚æˆ‘åªæ˜¯æƒ³ç»™è¯»è€…ä¸€ä¸ªåšå®çš„åŸºç¡€ï¼Œè¿™æ ·ä»–ä»¬å°±å¯ä»¥äº†è§£æ•´ä¸ªä¸»é¢˜ï¼Œå¹¶åœ¨ä»–ä»¬ç¢°å·§å–œæ¬¢çš„æƒ…å†µä¸‹è¿½æ±‚è‡ªå·±æ›´å…·ä½“çš„å…´è¶£ã€‚

Some of the things you might want to do afterwards include making your own:

åç»­ä½ å¯ä»¥æœç´¢å­¦ä¹ ä»¥ä¸‹ä¸œè¥¿

*   Programming language ç¼–ç¨‹è¯­è¨€
*   Virtual machine è™šæ‹Ÿæœº
*   Template engine æ¨¡æ¿å¼•æ“
*   Scripting language è„šæœ¬è¯­è¨€
*   DSL é¢†åŸŸç‰¹å®šè¯­è¨€
*   JSON parser JSON è§£æå™¨
*   Syntax checker è¯­æ³•æ£€æŸ¥å™¨
*   Synax highlighter è¯­æ³•é«˜äº®å™¨
*   Smart code renaming æ™ºèƒ½ä»£ç é‡å‘½å
*   Smart autocompleteâ€¦ æ™ºèƒ½è‡ªåŠ¨è¡¥å…¨
*   ..and more. The sky is the limit! æ›´å¤š...

Overview of our compiler ç¼–è¯‘å™¨æ¦‚è¿°
------------------------

Our compiler will mimic the most common compiler structure out there, and weâ€™ll boil it down to the very core of it. Our compiler will consist of three steps. The first step is transforming the input markdown string into a list of tokens.

æˆ‘ä»¬çš„ç¼–è¯‘å™¨å°†æ¨¡ä»¿æœ€å¸¸è§çš„ç¼–è¯‘å™¨ç»“æ„ï¼Œæˆ‘ä»¬ä¼šæŠŠå®ƒæµ“ç¼©åˆ°æœ€æ ¸å¿ƒçš„éƒ¨åˆ†ã€‚æˆ‘ä»¬çš„ç¼–è¯‘å™¨å°†åŒ…æ‹¬ä¸‰ä¸ªæ­¥éª¤ã€‚ç¬¬ä¸€æ­¥æ˜¯å°†è¾“å…¥çš„ Markdown å­—ç¬¦ä¸²è½¬æ¢ä¸ºtokenï¼ˆæ ‡è®°ï¼‰åˆ—è¡¨ã€‚

```
"_Hello,world!_" --> TOKENIZER --> [UNDERSCORE, TEXT="Hello,World!", UNDERSCORE]
```


A token is just a name for the basic building blocks of our language. For example an underscore, an asterisk, a new line, or just some words. This will make things easier for us later on.

token åªæ˜¯æˆ‘ä»¬è¯­è¨€çš„åŸºæœ¬æ„å»ºå—çš„åç§°ã€‚ä¾‹å¦‚ä¸‹åˆ’çº¿ã€æ˜Ÿå·ã€æ–°è¡Œæˆ–åªæ˜¯ä¸€äº›å•è¯ã€‚è¿™å°†ä½¿æˆ‘ä»¬ä»¥åçš„äº‹æƒ…å˜å¾—æ›´å®¹æ˜“ã€‚

```
[UNDERSCORE, TEXT="Hello,World!", UNDERSCORE] --> PARSER --> #<EmphasisText "Hello,World!">
```

Next, we take those tokens and pass them into a parser. That parser will give us a tree data-structure representing our tokens organized in certain way.

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è·å–è¿™äº› token å¹¶å°†å®ƒä»¬ä¼ é€’ç»™è§£æå™¨ã€‚
è§£æå™¨å°†ä¸ºæˆ‘ä»¬æä¾›ä¸€ä¸ªæ ‘æ•°æ®ç»“æ„ï¼Œè¡¨ç¤ºä»¥æŸç§æ–¹å¼ç»„ç»‡çš„ä»¤ç‰Œã€‚

```
#<EmphasisText "Hello,World!"> --> CODEGEN --> <em>Hello,World!</em>
```

Overall, the process looks like this:

æ€»çš„æ¥è¯´ï¼Œè¿™ä¸ªè¿‡ç¨‹æ˜¯è¿™æ ·çš„ï¼š

```ruby
"_Hello,world!_" --> TOKENIZER 
--> [UNDERSCORE, TEXT="Hello,World!", UNDERSCORE] --> PARSER 
--> #<EmphasisText "Hello,World!"> --> CODEGEN 
--> <em>Hello,World!</em>
```

You might think this is all quite complicated, but itâ€™s actually the most standard way of writing compilers. With this structure, we not only divide the problem into smaller chunks so itâ€™s easier to reason about and test, we can easily swap some parts around, for example, change the code generator to emit, for example, RTF documents instead of HTML documents. We could also write a new Tokenizer and Parser for a different language, and as long as the returned Abstract Syntax Tree is in the same format, we can still generate proper HTML.

æ‚¨å¯èƒ½è®¤ä¸ºè¿™ä¸€åˆ‡éƒ½éå¸¸å¤æ‚ï¼Œä½†å®ƒå®é™…ä¸Šæ˜¯ç¼–å†™ç¼–è¯‘å™¨çš„æœ€æ ‡å‡†æ–¹å¼ã€‚
ä½¿ç”¨è¿™ç§ç»“æ„ï¼Œæˆ‘ä»¬ä¸ä»…å°†é—®é¢˜åˆ†æˆæ›´å°çš„å—ä»¥ä¾¿æ›´å®¹æ˜“æ¨ç†å’Œæµ‹è¯•ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥è½»æ¾äº¤æ¢ä¸€äº›éƒ¨åˆ†ï¼Œä¾‹å¦‚ï¼Œæ›´æ”¹ä»£ç ç”Ÿæˆå™¨ä»¥å¯¼å‡ºRTF æ–‡æ¡£è€Œä¸æ˜¯ HTML æ–‡æ¡£ã€‚
æˆ‘ä»¬ä¹Ÿå¯ä»¥ä¸ºä¸åŒçš„è¯­è¨€ç¼–å†™ä¸€ä¸ªæ–°çš„ Tokenizer å’Œ Parserï¼Œåªè¦è¿”å›çš„æŠ½è±¡è¯­æ³•æ ‘æ˜¯ç›¸åŒçš„æ ¼å¼ï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥ç”Ÿæˆæ­£ç¡®çš„ HTMLã€‚

The Tokenizer æ ‡è®°ç”Ÿæˆå™¨
-------------

Letâ€™s start implementing! The first step in our compiler process is _tokenizing_ â€“ also called Lexical Analisys. Tokenizing is basically making sense of a bunch of characters by transforming them into Tokens. For example: `Hello_` could be transformed to `[<TEXT=HELLO>, <UNDERSCORE>]`, an array of plain old Ruby objects.

è®©æˆ‘ä»¬å¼€å§‹å®ç°å®ƒå§ï¼ç¬¬ä¸€æ­¥æ˜¯*æ ‡è®°åŒ–*â€”â€” ä¹Ÿç§°ä¸ºè¯æ³•åˆ†æï¼ˆLexical Analisysï¼‰ã€‚
æ ‡è®°åŒ–åŸºæœ¬ä¸Šæ˜¯é€šè¿‡å°†ä¸€å †å­—ç¬¦è½¬æ¢ä¸ºæ ‡è®°æ¥ç†è§£å®ƒä»¬ã€‚ä¾‹å¦‚ï¼š`Hello_`å¯ä»¥è½¬æ¢ä¸º`[<TEXT=HELLO>, <UNDERSCORE>]`ï¼Œä¸€ä¸ªæ™®é€šçš„æ—§ Ruby å¯¹è±¡æ•°ç»„ã€‚

Because we want to recognize just a part of markdown, letâ€™s start with some examples of the things we will match:

å› ä¸ºæˆ‘ä»¬åªæƒ³è¯†åˆ« markdown çš„ä¸€éƒ¨åˆ†ï¼Œè®©æˆ‘ä»¬ä»ä¸€äº›æˆ‘ä»¬å°†åŒ¹é…çš„ä¾‹å­å¼€å§‹ï¼š

```
A paragraph __with__ some *text*
```

As we are only going to match paragraphs, emphasized text and bold text â€” no links, lists, quotes, etc â€” it makes sense to have only the following tokens: `UNDERSCORE`; `STAR`; `NEWLINE`; `TEXT` and `EOF`.
So, for example, for the input `_Hello*` our tokenizer should return `[<UNDERSCORE>, <TEXT="Hello">, <STAR>]`.

ç”±äºæˆ‘ä»¬åªåŒ¹é…æ®µè½ã€æ–œä½“å¼ºè°ƒæ–‡æœ¬å’Œç²—ä½“æ–‡æœ¬ï¼Œæ²¡æœ‰é“¾æ¥ã€åˆ—è¡¨ã€å¼•å·ç­‰ï¼Œæ‰€ä»¥åªæœ‰ä»¥ä¸‹æ ‡è®°æ‰æœ‰æ„ä¹‰ï¼š`UNDERSCORE`; `STAR`; `NEWLINE`; `TEXT`å’Œ`EOF`ã€‚
å› æ­¤å¯¹äºè¾“å…¥`_Hello*`ï¼Œæˆ‘ä»¬çš„åˆ†è¯å™¨åº”è¯¥è¿”å›`[<UNDERSCORE>, <TEXT="Hello">, <STAR>]`ã€‚

Letâ€™s start with a test which defines what our Tokenizer should do. Weâ€™ll use [Minitest](https://github.com/seattlerb/minitest) for the specs.

The full source code for the compiler lives in [GitHub](https://github.com/beezwax/markdown-compiler); we encourage you to clone and play with it. The snippets displayed here wonâ€™t give you the whole picture of this particular compiler, they instead focus on explaining concepts so you can write your own.

è®©æˆ‘ä»¬ä»å®šä¹‰ Tokenizer åº”è¯¥åšä»€ä¹ˆçš„æµ‹è¯•å¼€å§‹ã€‚æˆ‘ä»¬å°†ä½¿ç”¨[Minitest](https://github.com/seattlerb/minitest)ä½œä¸ºè§„èŒƒã€‚

ç¼–è¯‘å™¨çš„å®Œæ•´æºä»£ç ä½äº[GitHub ä¸­](https://github.com/beezwax/markdown-compiler)ï¼›æˆ‘ä»¬é¼“åŠ±æ‚¨å…‹éš†å¹¶ä½¿ç”¨å®ƒã€‚æ­¤å¤„æ˜¾ç¤ºçš„ç‰‡æ®µä¸ä¼šä¸ºæ‚¨æä¾›æ­¤ç‰¹å®šç¼–è¯‘å™¨çš„å…¨è²Œï¼Œè€Œæ˜¯ä¸“æ³¨äºè§£é‡Šæ¦‚å¿µï¼Œä»¥ä¾¿æ‚¨å¯ä»¥ç¼–å†™è‡ªå·±çš„ç¼–è¯‘å™¨ã€‚

There are numerous ways to write tokenizers. Each one is different, tailored to specific needs. In this series Iâ€™ll use a rather simple, object-oriented approach with emphasis on readability and simplicity.

æœ‰å¾ˆå¤šæ–¹æ³•å¯ä»¥ç¼–å†™æ ‡è®°ç”Ÿæˆå™¨ã€‚æ¯ä¸€ç§éƒ½æ˜¯ä¸åŒçš„ï¼Œé’ˆå¯¹ç‰¹å®šéœ€æ±‚é‡èº«å®šåˆ¶ã€‚åœ¨æœ¬ç³»åˆ—ä¸­ï¼Œæˆ‘å°†ä½¿ç”¨ä¸€ç§ç›¸å½“ç®€å•çš„é¢å‘å¯¹è±¡çš„æ–¹æ³•ï¼Œé‡ç‚¹æ˜¯å¯è¯»æ€§å’Œç®€å•æ€§ã€‚

Weâ€™ll start by building a `Tokenizer` object, which will take a markdown input string and return a list of `Token` objects that have `type` and `value` attributes.

æˆ‘ä»¬å°†ä»æ„å»ºä¸€ä¸ª`Tokenizer`å¯¹è±¡å¼€å§‹ï¼Œè¯¥å¯¹è±¡å°† markdown å­—ç¬¦ä¸²ä½œä¸ºè¾“å…¥å¹¶è¿”å›å…·æœ‰`type`å’Œ`value`å±æ€§çš„`Token`å¯¹è±¡åˆ—è¡¨ã€‚

```ruby
class Token
  attr_reader :type, :value
  def initialize(type:, value:)
    @type = type
    @value = value
    raise InvalidTokenError if value.nil? || type.nil?
  end

  def self.null
    NullToken.new
  end

  def self.end_of_file
    Token.new(type: 'EOF', value: '')
  end

  def length
    value.length
  end

  def null?
    false
  end

  def present?
    true
  end

  def to_s
    "<type: #{type}, value: #{value}>"
  end
end
```

Weâ€™ll then use some `Scanner` objects to find tokens. Basically, weâ€™ll register scanners that each match specific tokens. Then we run the text through all the scanners and collect what they return. Weâ€™ll stop when something could not be matched or everything has been consumed.

ç„¶åæˆ‘ä»¬å°†ä½¿ç”¨ä¸€äº›`Scanner`å¯¹è±¡æ¥æŸ¥æ‰¾æ ‡è®°ã€‚æˆ‘ä»¬å°†æ³¨å†Œå¤šä¸ªåŒ¹é…ç‰¹å®šæ ‡è®°çš„ Scannerã€‚
ç„¶åæˆ‘ä»¬è®©æ‰€æœ‰æ–‡æœ¬é€šè¿‡æ‰€æœ‰ Scanner å¹¶æ”¶é›†å®ƒä»¬è¿”å›çš„å†…å®¹ã€‚å½“æŸäº›ä¸œè¥¿æ— æ³•åŒ¹é…æˆ–æ‰€æœ‰ä¸œè¥¿éƒ½è¢«æ¶ˆè€—æ‰æ—¶ï¼Œæˆ‘ä»¬å°†åœæ­¢ã€‚

```ruby
class Tokenizer
  TOKEN_SCANNERS = [
    SimpleScanner, # Recognizes simple one-char tokens like `_` and `*`
    TextScanner    # Recognizes everything but a simple token
  ].freeze

  def tokenize(plain_markdown)
    tokens_array = tokens_as_array(plain_markdown)
    TokenList.new(tokens_array)
  end  

  private

  def tokens_as_array(plain_markdown)
    if plain_markdown.nil? || plain_markdown == ''
      [Token.end_of_file]
    else
      token = scan_one_token(plain_markdown)
      [token] + tokens_as_array(plain_markdown[token.length..-1])
    end
  end

  def scan_one_token(plain_markdown)
    TOKEN_SCANNERS.each do |scanner|
      token = scanner.from_string(plain_markdown)
      return token unless token.null?
    end
    raise "The scanners could not match the given input: #{plain_markdown}"
  end
end
```



The method of interest here is `scan_one_token`. It takes a plain markdown string and returns a single token, matching the first character of the input string. To do so, it iterates though the scanners, and if the token matched is not null â€” i.e., if itâ€™s valid â€” it will return that token. Otherwise, it will keep trying scanners. We fail if we consume the whole array and return nothing.

è¿™é‡Œæœ‰æ„æ€çš„æ–¹æ³•æ˜¯`scan_one_token`ã€‚å®ƒæ¥å—ä¸€ä¸ª Markdown çº¯æ–‡æœ¬å­—ç¬¦ä¸²å¹¶è¿”å›ä¸€ä¸ªä¸è¾“å…¥å­—ç¬¦ä¸²çš„ç¬¬ä¸€ä¸ªå­—ç¬¦åŒ¹é…çš„æ ‡è®°ã€‚ä¸ºæ­¤ï¼Œå®ƒé€šè¿‡æ‰«æå™¨è¿›è¡Œè¿­ä»£ï¼Œå¦‚æœåŒ¹é…çš„æ ‡è®°ä¸ä¸ºç©ºâ€”â€”ä¹Ÿå°±æ˜¯å®ƒæœ‰æ•ˆâ€”â€”å®ƒå°†è¿”å›è¯¥æ ‡è®°ã€‚å¦åˆ™å®ƒå°†ç»§ç»­å°è¯•æ‰«æä»ªã€‚å¦‚æœæˆ‘ä»¬å¤„ç†äº†æ•´ä¸ªæ•°ç»„å¹¶ä¸”ä»€ä¹ˆéƒ½ä¸è¿”å›ï¼Œæˆ‘ä»¬å°±å¤±è´¥äº†ã€‚

The `tokens_as_array` method is a wrapper for our previous method. Itâ€™s a recursive function which calls `scan_one_token` until thereâ€™s no more string to send, or the `scan_one_token` method raises an error. This method also appends an end-of-file token, which will be used to mark the end of the token list.

`tokens_as_array`æ–¹æ³•æ˜¯æˆ‘ä»¬ä¹‹å‰æ–¹æ³•çš„å°è£…ã€‚è¿™æ˜¯ä¸€ä¸ªé€’å½’å‡½æ•°ï¼Œå®ƒä¼šè°ƒç”¨`scan_one_token`ç›´åˆ°æ²¡æœ‰æ›´å¤šçš„å­—ç¬¦ä¸²è¦å‘é€ï¼Œæˆ–è€…è¯¥`scan_one_token`æ–¹æ³•å¼•å‘é”™è¯¯ã€‚æ­¤æ–¹æ³•è¿˜é™„åŠ äº†ä¸€ä¸ªæ–‡ä»¶ç»“æŸæ ‡è®°ï¼Œç”¨äºæ ‡è®°æ ‡è®°åˆ—è¡¨çš„æœ«å°¾ã€‚

The `TokenList` class itself is just a convenient wrapper around a collection, so thereâ€™s not much point showing it here. Same for `Token` â€” itâ€™s just a data object with two attributes, `type` and `value`.

è¯¥`TokenList`æœ¬èº«æ˜¯é›†åˆçš„çš„å°è£…ï¼Œåœ¨è¿™é‡Œå±•ç¤ºä¹Ÿæ²¡æœ‰ä»€ä¹ˆæ„ä¹‰ã€‚å®ƒè·Ÿ`Token`ç›¸åŒï¼Œåªæ˜¯ä¸€ä¸ªå…·æœ‰`type`å’Œ`value`å±æ€§çš„æ•°æ®å¯¹è±¡.

Whatâ€™s now left to show you are the scanners. Hereâ€™s the first one, which matches single characters â€” canâ€™t get simpler than this!

ç°åœ¨è¦å‘æ‚¨å±•ç¤ºçš„æ˜¯æ‰«æå™¨ï¼Œå®ƒåŒ¹é…å•ä¸ªå­—ç¬¦ï¼Œæ²¡æœ‰æ¯”è¿™æ›´ç®€å•çš„äº†ï¼

```ruby
class SimpleScanner
  TOKEN_TYPES = {
    '_'  => 'UNDERSCORE',
    '*'  => 'STAR',
    "\n" => 'NEWLINE'
  }.freeze

  def self.from_string(plain_markdown)
    char = plain_markdown[0]
    Token.new(type: TOKEN_TYPES[char], value: char)
  rescue InvalidTokenError
    Token.null
  end
end
```



As you can see, all the work is performed in the `from_string` method. All scanners must implement this method. The method takes a plain markdown string as input and returns a single token, using some logic to determine whether it should match it or not. When matched, it returns a valid token. Otherwise, it returns a â€œnull tokenâ€. Note that a token knows when itâ€™s invalid â€” in this case when either the `type` or the `value` are empty â€” thatâ€™s the `InvalidTokenError` we are catching.

å¦‚æ‚¨æ‰€è§ï¼Œæ‰€æœ‰å·¥ä½œéƒ½åœ¨`from_string`æ–¹æ³•ä¸­æ‰§è¡Œã€‚æ‰€æœ‰æ‰«æå™¨éƒ½å¿…é¡»å®ç°æ­¤æ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†ä¸€ä¸ªæ™®é€šçš„ markdown å­—ç¬¦ä¸²ä½œä¸ºè¾“å…¥å¹¶è¿”å›ä¸€ä¸ªå•ä¸€çš„æ ‡è®°ï¼Œä½¿ç”¨ä¸€äº›é€»è¾‘æ¥ç¡®å®šå®ƒæ˜¯å¦åº”è¯¥åŒ¹é…å®ƒã€‚åŒ¹é…æ—¶ï¼Œå®ƒè¿”å›ä¸€ä¸ªæœ‰æ•ˆçš„æ ‡è®°ã€‚å¦åˆ™ï¼Œå®ƒè¿”å›ä¸€ä¸ªâ€œç©ºæ ‡è®°â€ã€‚è¯·æ³¨æ„ï¼Œæ ‡è®°çŸ¥é“ä½•æ—¶æ— æ•ˆâ€”â€”åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå½“ `type`æˆ– `value`ä¸ºç©ºæ—¶â€”â€”è¿™å°±æ˜¯`InvalidTokenError`æˆ‘ä»¬è¦æ•è·çš„ã€‚

> **NOTE** Null objects are an object-oriented pattern which is used to get rid of unwanted `if` statements and avoid possible nil reference errors. If youâ€™ve never heard of this before, you might want to check out [this other blog post](https://blog.beezwax.net/2016/03/25/avoid-nil-checks-code-confidently-be-happy/)
> 
> **æ³¨æ„**Null å¯¹è±¡æ˜¯ä¸€ç§é¢å‘å¯¹è±¡çš„æ¨¡å¼ï¼Œç”¨äºå»é™¤ä¸éœ€è¦çš„`if`è¯­å¥å¹¶é¿å…å¯èƒ½çš„ nil å¼•ç”¨é”™è¯¯ã€‚å¦‚æœæ‚¨ä»¥å‰ä»æœªå¬è¯´è¿‡è¿™ä¸ªï¼Œæ‚¨å¯èƒ½æƒ³æŸ¥çœ‹[å…¶ä»–åšå®¢æ–‡ç« ](https://blog.beezwax.net/2016/03/25/avoid-nil-checks-code-confidently-be-happy/)

Now onto the other scanner, `TextScanner`. This one is a bit more complicated but still quite simple:

ç°åœ¨çœ‹çœ‹å¦ä¸€å°æ‰«æå™¨`TextScanner`ï¼š

```ruby
class TextScanner < SimpleScanner
  def self.from_string(plain_markdown)
    text = plain_markdown
           .each_char
           .take_while { |char| SimpleScanner.from_string(char).null? }
           .join('')
    Token.new(type: 'TEXT', value: text)
  rescue InvalidTokenError
    Token.null
  end
end

# tipsï¼štake_whileç”¨æ³•ï¼š
# a = [1, 2, 3, 4, 5, 0]
# a.take_while {|i| i < 3}    #=> [1, 2]
```



We take advantage of Ruby functional-style list processing to fetch as many valid characters from the string as we can. We consider a character _valid_ when itâ€™s not matched by the `SimpleScanner`.

åˆ©ç”¨ Ruby å‡½æ•°å¼listå¤„ç†ï¼Œä»å­—ç¬¦ä¸²ä¸­è·å–å°½å¯èƒ½å¤šçš„æœ‰æ•ˆå­—ç¬¦ã€‚å½“ä¸€ä¸ªå­—ç¬¦æ²¡æœ‰è¢«`SimpleScanner`åŒ¹é…åˆ°æ—¶ï¼Œæˆ‘ä»¬æŠŠå®ƒè§†ä¸ºæœ‰æ•ˆçš„ã€‚

And thatâ€™s the gist of the Tokenizer! If you want to play around with it you should just `git clone git@github.com:beezwax/markdown-compiler.git` and play with it with your favorite editor.
Try running the tests with `rake test test/test_tokenizer.rb` and adding new characters to be recognized, like `(`, `)`, `[`, and `]`.

ä»¥ä¸Šå°±æ˜¯æ ‡è®°ç”Ÿæˆå™¨çš„è¦ç‚¹ï¼Œä½ å¯ä»¥ä¸‹è½½æºä»£ç æŸ¥çœ‹ï¼š`git clone git@github.com:beezwax/markdown-compiler.git`
è¿è¡Œæ–¹æ³•ï¼š`rake test test/test_tokenizer.rb`
ä½ ä¹Ÿå¯ä»¥å¢åŠ æ–°çš„å­—ç¬¦æ¥è¢«è¯†åˆ«ï¼Œæ¯”å¦‚ï¼š `(`, `)`, `[`, and `]`ã€‚

You did it!

If youâ€™ve followed along, congrats! Youâ€™ve taken the first step towards writing a compiler. For now, you can relax, pat yourself on the back and sip some coffee. Next time, weâ€™ll talk about _Parsing_. Weâ€™ll learn about Grammars, Formal Languages and Abstract Syntax Trees. Donâ€™t worry â€” they are not as scary as they sound.

å¦‚æœä½ ä¸€ç›´è·Ÿç€åšï¼Œæ­å–œï¼æ‚¨å·²ç»è¿ˆå‡ºäº†ç¼–å†™ç¼–è¯‘å™¨çš„ç¬¬ä¸€æ­¥ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è®¨è®ºå¦‚ä½•è§£æã€‚æˆ‘ä»¬å°†å­¦ä¹ è¯­æ³•ã€å½¢å¼è¯­è¨€å’ŒæŠ½è±¡è¯­æ³•æ ‘ã€‚åˆ«æ‹…å¿ƒâ€”â€”å®ƒä»¬å¹¶ä¸åƒå¬èµ·æ¥é‚£ä¹ˆå¯æ€•ã€‚

Here are [Part 2, Parsing/Implementation](https://blog.beezwax.net/2017/08/10/writing-a-markdown-compiler-part-2/) and [Part 3, Code Generation](https://blog.beezwax.net/2018/05/25/writing-a-markdown-compiler-part-3/).



# Part2



Hello, and welcome to the second part of the Writing a Markdown Compiler series! In case youâ€™ve need it, here is [Part 1, Intro/Tokenizer](https://blog.beezwax.net/2017/07/07/writing-a-markdown-compiler/) and [Part 3, Code Generation](https://blog.beezwax.net/2018/05/25/writing-a-markdown-compiler-part-3/).
æ¬¢è¿æ¥åˆ°ç¼–å†™Markdownç¼–è¯‘å™¨ç³»åˆ—çš„ç¬¬äºŒéƒ¨åˆ†! 

In this part weâ€™ll talk about the second step in compiling: Parsing â€“ also known as Syntactic Analysis. This part has a bit more theory, so it might take some time to digest. Sip some coffee, relax, take yout time, and as long as you donâ€™t rush it youâ€™ll find itâ€™s not hard at all. 
åœ¨è¿™éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†è®¨è®ºç¼–è¯‘çš„ç¬¬äºŒæ­¥ï¼šè§£æï¼Œè¯´é«˜çº§ç‚¹å«å¥æ³•åˆ†æã€‚
è¿™ä¸€éƒ¨åˆ†æœ‰æ›´å¤šçš„ç†è®ºï¼Œæ‰€ä»¥å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´æ¥æ¶ˆåŒ–ã€‚å–æ¯å¥¶èŒ¶ï¼Œæ”¾æ¾ä¸€ä¸‹ï¼Œæ…¢æ…¢æ¥ï¼Œä¸è¦æ…Œã€‚ 

If you recall from our first part, we talked about Tokenizing, which is making sense of a bunch of characters by transforming them into tokens. Parsing is organizing those tokens into a tree data structure called **Abstract Syntax Tree**, or AST for short.
å¦‚æœä½ è¿˜è®°å¾—æˆ‘ä»¬çš„ç¬¬ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬è°ˆåˆ°äº†æ ‡è®°åŒ–ï¼ˆTokenizingï¼‰ï¼Œä¹Ÿå°±æ˜¯é€šè¿‡æŠŠä¸€æ®µå­—ç¬¦ä¸²è½¬åŒ–ä¸ºTokenå¯¹è±¡ã€‚è§£ææ˜¯å°†è¿™äº›Tokenå¯¹è±¡ç»„ç»‡æˆä¸€ä¸ªæ ‘çš„æ•°æ®ç»“æ„ï¼Œç§°ä¸ºæŠ½è±¡è¯­æ³•æ ‘ï¼Œç®€ç§°ASTã€‚

For example, say were to design a language where you can assign a variable like this:
ä¾‹å¦‚ï¼Œå‡è®¾è¦è®¾è®¡ä¸€ç§è¯­è¨€ï¼Œä½ å¯ä»¥åƒè¿™æ ·åˆ†é…ä¸€ä¸ªå˜é‡ï¼š

```
foo = 1
```

è¯‘è€…æ³¨ï¼šå¼ ä¸‰ = 1

We could establish that assignment are made of a word token, an equals token, and a number token. The following are *invalid* assignments:
ä¸Šé¢çš„ä¹Ÿå«èµ‹å€¼ï¼Œèµ‹å€¼æ˜¯ç”±ä¸€ä¸ªå•è¯æ ‡è®°ã€ä¸€ä¸ªç­‰å·æ ‡è®°å’Œä¸€ä¸ªæ•°å­—æ ‡è®°ç»„æˆã€‚ä»¥ä¸‹æ˜¯æ— æ•ˆçš„èµ‹å€¼ï¼š

```
foo = bar # expects a number on the right hand side of the equation
foo       # no equals, no number
foo =     # nothing here!
= foo     # nothing on the left hand side
```

You can see we only accept a small numbers of token sequences. In fact, the accepted sequences must be carefully ordered in order to be valid. A common solution to this problem â€“ matching sequences â€“ is regular expressions. A not-so-common solution is writing a Parser.
ä½ å¯ä»¥çœ‹åˆ°æˆ‘ä»¬åªæ¥å—å°‘é‡çš„æ ‡è®°åºåˆ—ã€‚äº‹å®ä¸Šï¼Œæ‰€æ¥å—çš„åºåˆ—å¿…é¡»ç»è¿‡ä»”ç»†çš„æ’åºï¼Œæ‰èƒ½æœ‰æ•ˆã€‚åŒ¹é…åºåˆ—çš„ä¸€ä¸ªå¸¸è§è§£å†³æ–¹æ¡ˆæ˜¯ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ï¼Œä¸€ä¸ªæ›´æœ‰é€¼æ ¼çš„è§£å†³æ–¹æ¡ˆæ˜¯ç¼–å†™ä¸€ä¸ªè§£æå™¨ã€‚

The parser itself is just a program which returns true if a sequence is valid, and false otherwise. Our parser will also return a Node of an AST, weâ€™ll learn more about that in a bit.
è§£æå™¨æœ¬èº«åªæ˜¯ä¸€ä¸ªç¨‹åºï¼Œå¦‚æœä¸€ä¸ªåºåˆ—æ˜¯æœ‰æ•ˆçš„ï¼Œå°±è¿”å›çœŸï¼Œå¦åˆ™å°±è¿”å›å‡ã€‚æˆ‘ä»¬çš„è§£æå™¨ä¹Ÿå°†è¿”å›ASTçš„ä¸€ä¸ªèŠ‚ç‚¹ã€‚

## Theory and Grammar ç†è®ºå’Œè¯­æ³•

Now itâ€™s time for a little theory. Donâ€™t worry, I promise it wonâ€™t be that bad: A **grammar** is a set of rules which together define all possible valid character sequences. They look like this:
ç°åœ¨åˆ°äº†åœ¨ä¸‹è®²ç†è®ºçš„æ—¶å€™äº†ã€‚åˆ«æ‹…å¿ƒï¼Œæˆ‘ä¿è¯ä¸‹é¢çš„çŒ´å­éƒ½èƒ½çœ‹æ‡‚ã€‚
è¯­æ³•æ˜¯å¾ˆå¤šä¸ªè§„åˆ™çš„é›†åˆï¼Œå®ƒä»¬å…±åŒå®šä¹‰äº†æ‰€æœ‰å¯èƒ½çš„æœ‰æ•ˆå­—ç¬¦åºåˆ—ã€‚å®ƒä»¬çœ‹èµ·æ¥åƒè¿™æ ·

```ruby
RuleName := SomeOtherRule A_TERMINAL
```

Rules can consist only of other rules and/or a terminal. For example, if we wanted to match a tiny language `L = { a, b, c }` we could write:
è§„åˆ™å¯ä»¥åªç”±å…¶ä»–è§„åˆ™å’Œ/æˆ–ä¸€ä¸ªç»ˆç«¯ç»„æˆã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æƒ³åŒ¹é…ä¸€ä¸ªå¾®å°çš„è¯­è¨€L = { a, b, c }ï¼Œæˆ‘ä»¬å¯ä»¥å†™ï¼š

```
Start := a
       | b
       | c
```

`L = { a, b, c }` can be read as â€œThe language named L is made of 3 elements: `a`, `b`, and `c`â€œ.
`L = { a, b, c }` å¯ä»¥ç†è§£æˆ "åå­—å«Lçš„è¯­è¨€ç”±aã€bã€c è¿™3ä¸ªå…ƒç´ ç»„æˆ"ã€‚

In this case, `a`, `b` and `c` are all terminals, they match themselves. We could use any symbol to represent *or*, we used `|` as itâ€™s quite common. There is not an unified grammar representation â€“ every library makes itâ€™s own choices â€“ nevertheless they all come from something called [Backus Naur Form](https://en.wikipedia.org/wiki/Backusâ€“Naur_form).
aã€bå’Œcéƒ½æ˜¯ç»ˆç»“ç¬¦ï¼Œå®ƒä»¬ä¸è‡ªå·±åŒ¹é…ã€‚æˆ‘ä»¬å¯ä»¥ç”¨ä»»ä½•ç¬¦å·æ¥è¡¨ç¤º*æˆ–*ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨äº†`|`ï¼Œå› ä¸ºå¾ˆå¤šç¼–ç¨‹è¯­è¨€ä¹Ÿè¿™æ ·ã€‚è™½ç„¶æ²¡æœ‰ä¸€ä¸ªç»Ÿä¸€çš„è¯­æ³•è¡¨ç¤ºï¼Œä½†æ˜¯å¾ˆå¤šéƒ½æ¥è‡ªä¸€ä¸ªå«Backus Naur Formçš„ä¸œè¥¿ã€‚

Weâ€™ll also use something called *Kleene star*:
æˆ‘ä»¬è¿˜å°†ä½¿ç”¨ä¸€ç§å«åšå…‹è±å°¼æ˜Ÿå·çš„ä¸œè¥¿ï¼ˆæ³¨ï¼šKleene æ˜Ÿå·ï¼Œæˆ–ç§°Kleene é—­åŒ…ï¼Œåœ¨æ•°å­¦ä¸Šæ˜¯ä¸€ç§é€‚ç”¨äºå­—ç¬¦ä¸²æˆ–ç¬¦å·åŠå­—å…ƒçš„é›†åˆçš„ä¸€å…ƒè¿ç®—ã€‚å½“ Kleene æ˜Ÿå·è¢«åº”ç”¨åœ¨ä¸€ä¸ªé›†åˆ`V`æ—¶ï¼Œå†™æ³•æ˜¯`V*`ã€‚å®ƒè¢«å¹¿æ³›ç”¨äºæ­£åˆ™è¡¨è¾¾å¼ï¼‰

```c
A := a*
```

It simply means: Match this 0 or more times. It will match the empty string `""`, `a`, `aa`, `aaa` and so on. A very similar helper is *Kleen plus*, which means: Match this 1 or more times.
å®ƒçš„æ„æ€å¾ˆç®€å•ã€‚åŒ¹é…è¿™ä¸ªå­—ç¬¦0æ¬¡æˆ–æ›´å¤šæ¬¡ã€‚å®ƒå°†åŒ¹é…ç©ºå­—ç¬¦ä¸²`""`ã€`a`ã€`aa`ã€`aaa`ï¼Œä»¥æ­¤ç±»æ¨ã€‚ä¸€ä¸ªéå¸¸ç±»ä¼¼çš„ç©æ„å„¿æ˜¯ *Kleen plus*ï¼Œå®ƒçš„æ„æ€æ˜¯åŒ¹é…1æ¬¡æˆ–æ›´å¤šæ¬¡ã€‚

```
B := b+
```

Will match `b`, `bb`, `bbb` and so on, but not the empty string.
ä¸Šé¢çš„å°†åŒ¹é…bã€bbã€bbbç­‰ï¼Œä½†ä¸åŒ¹é…ç©ºå­—ç¬¦ä¸²ã€‚

The order in which the grammar tries the rules is not formally defined, any possible match is a valid match. For example, consider the following grammar:
è¯­æ³•å°è¯•çš„è§„åˆ™çš„é¡ºåºæ²¡æœ‰æ­£å¼å®šä¹‰ï¼Œä»»ä½•å¯èƒ½çš„åŒ¹é…éƒ½æ˜¯æœ‰æ•ˆçš„åŒ¹é…ï¼Ÿå…ˆçœ‹çœ‹ä¸‹é¢çš„è¯­æ³•ï¼š

```
Start := "ab" A
       | "aba"
A     := "a"
```

In that grammar, we have two ways of generating â€˜abaâ€™, one is by using the first branch of the *or*, and the other is using the second brach.
åœ¨è¯¥è¯­æ³•ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸¤ç§äº§ç”Ÿ'aba'çš„æ–¹å¼ï¼Œä¸€ç§æ˜¯ä½¿ç”¨orçš„ç¬¬ä¸€ä¸ªåˆ†æ”¯ï¼Œå¦ä¸€ç§æ˜¯ä½¿ç”¨ç¬¬äºŒä¸ªåˆ†æ”¯ã€‚

In our implementation weâ€™ll use a top-down approach and just match the first branch. This means we would ignore the second branch, so be careful with your rules.
åœ¨æˆ‘ä»¬çš„å®ç°ä»£ç ä¸­ï¼Œå°†ä½¿ç”¨è‡ªä¸Šè€Œä¸‹çš„æ–¹æ³•ï¼ŒåªåŒ¹é…ç¬¬ä¸€ä¸ªåˆ†æ”¯ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬ä¼šå¿½ç•¥ç¬¬äºŒä¸ªåˆ†æ”¯ï¼Œæ‰€ä»¥è¦æ³¨æ„ä½ çš„è§„åˆ™ã€‚

Languages generated by a grammar are called *formal languages*, you already know several formal languages, some of them are HTML, XML, CSS, JavaScript, Ruby, Swift, Java, and C.
ç”±è¯­æ³•ç”Ÿæˆçš„è¯­è¨€è¢«ç§°ä¸ºå½¢å¼è¯­è¨€ï¼Œæ¯”å¦‚HTMLã€XMLã€CSSã€JavaScriptã€Rubyã€Swiftã€Javaå’ŒCã€‚

Also, we wonâ€™t write just any grammar, weâ€™ll limit our rules in the grammar a bit, that way weâ€™ll only match Context-Free Languages. Why? Because they represent the best compromise between power of expression and ease of implementation. [You can learn more about grammars here](http://www.cs.nuim.ie/~jpower/Courses/Previous/parsing/node21.html).
æˆ‘ä»¬ä¹Ÿä¸ä¼šéšä¾¿å†™è¯­æ³•ï¼Œæˆ‘ä»¬ä¼šå¯¹è¯­æ³•ä¸­çš„è§„åˆ™åšä¸€äº›é™åˆ¶ï¼Œè¿™æ ·æˆ‘ä»¬å°±åªèƒ½åŒ¹é…ä¸Šä¸‹æ–‡æ— å…³ï¼ˆè·Ÿè¯­å¢ƒæ— å…³çš„ï¼‰è¯­è¨€ã€‚ä¸ºä»€ä¹ˆå‘¢ï¼Ÿå› ä¸ºå®ƒä»¬ä»£è¡¨äº†è¡¨è¾¾èƒ½åŠ›å’Œæ˜“äºå®ç°ä¹‹é—´çš„æœ€ä½³å¦¥åã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œäº†è§£æ›´å¤šå…³äºè¯­æ³•çš„ä¿¡æ¯ã€‚

Which limitations are we talking about exactly? Not many really, we just need to avoid left-recursion:
åˆ°åº•æ˜¯å“ªäº›é™åˆ¶å‘¢ï¼Ÿå…¶å®ä¸å¤šï¼Œæˆ‘ä»¬åªæ˜¯éœ€è¦é¿å…å·¦é€’å½’ï¼ˆå·¦æ—‹ï¼Ÿï¼‰ã€‚

```
Foo := Foo "ab"
     | "ab"
```

A rule which calls itself before calling another rule or a terminal. Why this limitation? Well, one is because itâ€™s harder to implement. Because weâ€™ll use functions to implement rules, the implemenation of a left-recursive rule looks like this:
å³ä¸€ä¸ªè§„åˆ™åœ¨è°ƒç”¨å¦ä¸€ä¸ªè§„åˆ™æˆ–ç»ˆç»“ç¬¦ä¹‹å‰è°ƒç”¨è‡ªå·±ã€‚ä¸ºä»€ä¹ˆæœ‰è¿™ç§é™åˆ¶ï¼Ÿå—¯ï¼Œä¸€æ˜¯å› ä¸ºå®ƒæ›´éš¾å®ç°ã€‚å› ä¸ºæˆ‘ä»¬è¦ç”¨å‡½æ•°æ¥å®ç°è§„åˆ™ï¼Œæ‰€ä»¥å·¦é€’å½’è§„åˆ™çš„å®ç°çœ‹èµ·æ¥åƒè¿™æ ·ã€‚

```ruby
def my_rule
  if my_rule # infinite loop here è¿™é‡Œæ— é™å¾ªç¯äº†
    do something
  else
    do something else
  end
end
```

Weâ€™ve got an infinite loop! The good news is that all grammars with left-recursion [can be written as a different equivalent grammar without left-recursion](http://www.csd.uwo.ca/~moreno/CS447/Lectures/Syntax.html/node8.html). In the next section weâ€™ll convert a left-recursive grammar into a non-left-recursive one.
æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªæ— é™å¾ªç¯! å¥½æ¶ˆæ¯æ˜¯ï¼Œæ‰€æœ‰å«æœ‰å·¦é€’å½’çš„è¯­æ³•éƒ½å¯ä»¥å†™æˆæ²¡æœ‰å·¦é€’å½’çš„å…¶ä»–ç­‰ä»·è¯­æ³•ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æŠŠä¸€ä¸ªå·¦é€’å½’çš„è¯­æ³•è½¬æ¢æˆä¸€ä¸ªéå·¦é€’å½’çš„è¯­æ³•ã€‚

Just one more thing before we move on, I just want to show you how to to evaluate a grammar *by hand*. Letâ€™s this tiny grammar as an example:
åœ¨ç»§ç»­ä¹‹å‰æˆ‘æƒ³å‘Šè¯‰ä½ å¦‚ä½•è¯„ä¼°ä¸€ä¸ªè¯­æ³•ã€‚ä»¥ä¸‹é¢è¿™ä¸ªå°å°çš„è¯­æ³•ä¸ºä¾‹ï¼š

```
Assign     := Identifier EQUALS Number
Identifier := WORD
Number     := NUMBER
```

In the grammar above, I want to match an Identifier rule, a token of type EQUALS (also known as terminal), and a Number. As you can see, weâ€™ve defined them using some building blocks called Terminals or Tokens. In our code, weâ€™ll tell the *WORD* token to match `[a-z]+` and the *NUMBER* token will match just [0-9].
åœ¨ä¸Šé¢çš„è¯­æ³•ä¸­ï¼Œæˆ‘æƒ³åŒ¹é…ä¸€ä¸ªæ ‡è¯†ç¬¦è§„åˆ™ã€ä¸€ä¸ªEQUALSï¼ˆç­‰å·ï¼‰ç±»å‹çš„æ ‡è®°ï¼ˆä¹Ÿè¢«ç§°ä¸ºç»ˆç»“ç¬¦ï¼‰å’Œä¸€ä¸ªæ•°å­—ã€‚æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€äº›è¢«ç§°ä¸ºç»ˆç»“ç¬¦æˆ–æ ‡è®°çš„æ„å»ºå—æ¥å®šä¹‰å®ƒä»¬ã€‚åœ¨æˆ‘ä»¬çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬å°†å‘Šè¯‰WORDæ ‡è®°åŒ¹é…`[a-z]+`ï¼ŒNUMBERæ ‡è®°å°†åªåŒ¹é…`[0-9]`ï¼Œè¾¾åˆ°è·Ÿæ­£åˆ™è¡¨è¾¾å¼ä¸€æ ·çš„æ•ˆæœã€‚

To try out this grammar, all we need to know is the substitution model. We just replace rules with their definition until all we have are terminals. Letâ€™s say I want to match `foo = 1`. We must start from the initial rule and see if we can get to that:
ä¸ºäº†å°è¯•è¿™ä¸ªè¯­æ³•ï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“çš„æ˜¯æ›¿ä»£æ¨¡å‹ã€‚æˆ‘ä»¬åªéœ€ç”¨å®ƒä»¬çš„å®šä¹‰æ¥æ›¿æ¢è§„åˆ™ï¼Œç›´åˆ°æˆ‘ä»¬æ‰€æ‹¥æœ‰çš„éƒ½æ˜¯ç»ˆç»“ç¬¦ã€‚æ¯”å¦‚æˆ‘æƒ³åŒ¹é…`foo = 1`ã€‚æˆ‘ä»¬å¿…é¡»ä»æœ€åˆçš„è§„åˆ™å¼€å§‹ï¼Œçœ‹çœ‹æˆ‘ä»¬æ˜¯å¦èƒ½è¾¾åˆ°è¿™ä¸ªç›®çš„ã€‚

```
Assign := Identifier EQUALS Number
       := WORD EQUALS NUMBER
       := foo EQUALS NUMBER # foo is a valid workd token, we can replace it
       := foo = NUMBER      # = is a valid equals token
       := foo = 1           # 1 is a valid number token
```

We were able to get to `foo = 1`, so it belongs to our language.
æˆ‘ä»¬èƒ½å¤Ÿå¾—åˆ°foo = 1ï¼Œæ‰€ä»¥å®ƒå±äºæˆ‘ä»¬çš„è¯­è¨€ã€‚

## On Abstract Syntax Trees å…³äºæŠ½è±¡è¯­æ³•æ ‘

Now, just some more theory before I let you go ğŸ™‚ The whole point of the grammar is to get an Abstract Syntax Tree representation â€“ or AST for short, of our input. For example, a markdown grammar might parse `hello __world__.` as:
ç»§ç»­ç»§ç»­ï¼è¯­æ³•çš„å…¨éƒ¨æ„ä¹‰åœ¨äºå¾—åˆ°ä¸€ä¸ªæŠ½è±¡è¯­æ³•æ ‘ï¼ˆç®€ç§°ASTï¼‰çš„è¡¨ç¤ºï¼Œæˆ‘ä»¬çš„è¾“å…¥ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªmarkdownè¯­æ³•å¯èƒ½ä¼šæŠŠ`hello __world__`è§£æä¸º

```
               [PARAGRAPH]
                   |
                   v
      +-------[SENTENCES]-----------+
      |               |             |
      v               v             v
[TEXT="hello "] [BOLD="world"] [TEXT="."]
```

> **NOTE** If youâ€™ve never seen a tree data structure before, you might [want to check that out](https://en.wikipedia.org/wiki/Tree_(data_structure)).
> **æ³¨æ„** å¦‚æœä½ ä¸äº†è§£æ ‘è¿™ç§æ•°æ®ç»“æ„ï¼Œä½ å¯ä»¥çœ‹çœ‹è¿™ä¸ªã€‚

Our parent node is PARAGRAPH. That node has a single child, SENTENCES, which in turn has 3 children nodes, TEXT, BOLD and another TEXT. The starting rule in our parser will be the top-most parent in our tree.
ä¸Šé¢è¿™æ£µæ ‘çˆ¶èŠ‚ç‚¹æ˜¯PARAGRAPHï¼Œè¯¥èŠ‚ç‚¹æœ‰ä¸€ä¸ªå„¿å­èŠ‚ç‚¹SENTENCESï¼Œå®ƒåˆæœ‰ä¸‰ä¸ªå­™å­èŠ‚ç‚¹TEXTã€BOLDå’Œå¦ä¸€ä¸ªTEXTã€‚æˆ‘ä»¬è§£æå™¨ä¸­çš„èµ·å§‹è§„åˆ™æ˜¯æ ‘ä¸­æœ€é¡¶å±‚çš„çˆ¶èŠ‚ç‚¹ã€‚

The thing about getting a tree out of a grammar is that we can remove ambiguity. Consider the following grammar:
ä»è¯­æ³•ä¸­å¾—åˆ°ä¸€æ£µæ ‘çš„å¥½å¤„æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥æ¶ˆé™¤æ­§ä¹‰ã€‚çœ‹çœ‹ä¸‹é¢çš„è¯­æ³•

```
Start    := Binop
Binop    := Binop Operator Binop
          | Number
Operator := + | - | * | /
Number   := 0 | 1 | 2 | ... | 9
```

If we were to manually build an AST for `2 + 2 - 4`, we get
å¦‚æœæˆ‘ä»¬æ‰‹åŠ¨æ„å»º`2 + 2 - 4`çš„ASTï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°

```
                +------[START]--------+
                |         |           |
                v         v           v
             [BINOP] [OPERATOR=-] [NUMBER=4]
                |
   +------------+----------+
   |            |          |
   v            v          v
[NUMBER=2] [OPERATOR=+] [NUMBER=2]
```

The way to read an AST is reading all the leafs of the tree â€“ nodes without children â€“ from left ro right. If we do that, you can see we matched `(2 + 2) - 4`. The problem, is that an equally valid representation could be:
é˜…è¯»ASTçš„æ–¹æ³•æ˜¯é˜…è¯»æ ‘çš„æ‰€æœ‰å¶å­èŠ‚ç‚¹ä»å·¦åˆ°å³ï¼ˆå¶å­èŠ‚ç‚¹æ˜¯æ²¡æœ‰å­èŠ‚ç‚¹çš„èŠ‚ç‚¹ï¼‰ã€‚å¦‚æœæŒ‰ç…§è¿™ç§é˜…è¯»æ–¹æ³•å¯ä»¥çœ‹åˆ°æˆ‘ä»¬åŒ¹é…äº†`(2 + 2)- 4`

> è¯‘è€…æ³¨ï¼šæœ‰ç‚¹åƒ[å‚åºéå†](https://leetcode-cn.com/problems/vertical-order-traversal-of-a-binary-tree/)

```
   +------[START]----------+
   |          |            |
   v          v            v
[NUMBER=2] [OPERATOR=+] [BINOP]
                           |
              +------------+----------+
              |            |          |
              v            v          v
           [NUMBER=2] [OPERATOR=-] [NUMBER=4]
```

This time, we end up with `2 + (2 - 4)`. We have two possible ASTs to choose from! Because we want our programs to be deterministic (given the same input, always return the same output), looks like we have some issues.
è¿™æ¬¡æˆ‘ä»¬çš„ç»“æœæ˜¯`2 + (2 - 4)`ï¼Œæœ‰äº†ä¸¤ä¸ªASTå¯ä»¥é€‰æ‹©ï¼Œä½†æ˜¯æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç¨‹åºæ˜¯ç¡®å®šæ€§çš„ï¼ˆç»™å®šç›¸åŒçš„è¾“å…¥ï¼Œæ€»æ˜¯è¿”å›ç›¸åŒçš„è¾“å‡ºï¼‰ï¼Œæ˜¯ä¸æ˜¯æˆ‘ä»¬æœ‰ä¸€äº›é—®é¢˜å‘¢ï¼Ÿ

Well not really. Luckly for us, we happen to use left-recursive grammars. Those grammars have a nice property which is they never have ambiguity! Letâ€™s transform the old grammar:
å…¶å®ä¸ç„¶ã€‚æˆ‘ä»¬å¾ˆå¹¸è¿ï¼Œç¢°å·§ä½¿ç”¨äº†å·¦é€’å½’è¯­æ³•ã€‚è¿™äº›è¯­æ³•æœ‰ä¸€ä¸ªå¾ˆå¥½çš„ç‰¹æ€§ï¼Œå°±æ˜¯å®ƒä»¬æ°¸è¿œä¸ä¼šæœ‰æ­§ä¹‰ã€‚è®©æˆ‘ä»¬è½¬æ¢ä¸€ä¸‹æ—§çš„è¯­æ³•ï¼š

```
Start          := Binop
Binop          := Subtraction
Subtraction    := Adition "-" Binop
                | Adition
Adition        := Division "+" Binop
                | Division
Division       := Multiplication "/" Binop
                | Multiplication
Multiplication := Number "*" Binop
                | Number
Number         := 0 | 1 | 2 | ... | 9
```

As you can see, we explicitly set the order of the operations to be performed, which in this case is multiplication, division, adition, and subtraction â€“ just like C. The generated AST will now always be the same. Letâ€™s see the way the grammar evaluates `2 + 2` so you get the hang of this trick:
åœ¨ä¸Šé¢æˆ‘ä»¬æ˜ç¡®åœ°è®¾ç½®äº†è¦æ‰§è¡Œçš„åŠ å‡ä¹˜é™¤æ“ä½œç¬¦çš„é¡ºåºï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯ä¹˜æ³•ã€é™¤æ³•ã€åŠ æ³•å’Œå‡æ³•ï¼Œå°±åƒCè¯­è¨€ä¸€æ ·ã€‚è®©æˆ‘ä»¬çœ‹çœ‹è¯­æ³•å¯¹`2 + 2`çš„æ±‚å€¼æ–¹å¼ï¼Œè¿™æ ·ä½ å°±èƒ½æŒæ¡è¿™ä¸ªæŠ€å·§äº†

```
;; Parsing 2 + 2
Start := Binop
      := Subtraction
      := Adition
      := Division "+" Binop
      := Multiplication "+" Binop
      := Number "+" Binop
      := 2 "+" Binop
      := 2 "+" Subtraction
      := 2 "+" Adition
      := 2 "+" Division
      := 2 "+" Multiplication
      := 2 "+" Number
      := 2 "+" 2
```

This trick of transforming a left-recursive grammar to a non-left-recursive grammar works for all grammars. [For more on this, you might want to check this article](http://www.csd.uwo.ca/~moreno/CS447/Lectures/Syntax.html/node8.html).
è¿™ç§å°†å·¦é€’å½’è¯­æ³•è½¬æ¢ä¸ºéå·¦é€’å½’è¯­æ³•çš„æŠ€å·§å¯¹æ‰€æœ‰çš„è¯­æ³•éƒ½é€‚ç”¨ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è€ƒè¿™ç¯‡æ–‡ç« ã€‚

## A simple Markdown grammar ä¸€ä¸ªç®€å•çš„Markdownè¯­æ³•

Okay, enough theory, letâ€™s start coding already! This is the grammar weâ€™ll implement:
å¥½äº†ï¼Œç†è®ºå¤Ÿäº†ï¼Œè®©æˆ‘ä»¬å¼€å§‹ç¼–ç å§ï¼è¿™æ˜¯æˆ‘ä»¬è¦å®ç°çš„è¯­æ³•ï¼š

```
Body               := Paragraph*

Paragraph          := SentenceAndNewline
                    | SentenceAndEOF
                    
SentenceAndNewline := Sentence+ NEWLINE NEWLINE
SentencesAndEOF    := Sentence+ NEWLINE EOF
                    | Sentence+ EOF
                    
Sentence           := EmphasizedText
                    | BoldText
                    | Text
                    
EmphasizedText     := UNDERSCORE BoldText UNDERSCORE

BoldText           := UNDERSCORE UNDERSCORE TEXT UNDERSCORE UNDERSCORE
                    | STAR STAR TEXT STAR STAR
                    
Text               := TEXT
```

Our starting rule is `Body`, which just matches 0 or more Paragraphs. Each paragraph is made of either a `SentenceAndNewline` rule or a `SentenceAndEOF` rule. A Sentence is just text, bold text, or emphasized text.
æˆ‘ä»¬çš„èµ·å§‹è§„åˆ™æ˜¯Bodyï¼Œå®ƒåªåŒ¹é…0ä¸ªæˆ–å¤šä¸ªæ®µè½ã€‚æ¯ä¸ªæ®µè½éƒ½æ˜¯ç”±ä¸€ä¸ª `SentenceAndNewline` è§„åˆ™æˆ– `SentenceAndEOF` è§„åˆ™ç»„æˆã€‚å¥å­åªæ˜¯æ–‡æœ¬ã€ç²—ä½“å­—æˆ–å¼ºè°ƒæ–‡å­—ã€‚

Note that the `Text` rule seems quite silly. Itâ€™s just so it makes the implementation easier, we could easily get rid of it and just replace it with `TEXT`.
è¯·æ³¨æ„ï¼Œæ–‡æœ¬è§„åˆ™ä¼¼ä¹å¾ˆå‚»ã€‚è¿™åªæ˜¯ä¸ºäº†ä½¿å®ç°æ›´å®¹æ˜“ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°æ‘†è„±å®ƒï¼Œç›´æ¥ç”¨TEXTæ¥ä»£æ›¿å®ƒã€‚

## Implementation å®ç°

The approach weâ€™ll take is creating several small objects, each matching a single rule in our grammar. Weâ€™ll call those objects `parsers`, each `parser` might call other parsers in order to match the rule, including itself.
æˆ‘ä»¬å°†é‡‡å–çš„æ–¹æ³•æ˜¯åˆ›å»ºå‡ ä¸ªå°å¯¹è±¡ï¼Œæ¯ä¸ªå¯¹è±¡ä¸æˆ‘ä»¬è¯­æ³•ä¸­çš„ä¸€æ¡è§„åˆ™ç›¸åŒ¹é…ã€‚æˆ‘ä»¬å°†è¿™äº›å¯¹è±¡ç§°ä¸ºè§£æå™¨ï¼Œæ¯ä¸ªè§£æå™¨å¯èƒ½ä¼šè°ƒç”¨å…¶ä»–è§£æå™¨ä»¥åŒ¹é…è§„åˆ™ï¼ŒåŒ…æ‹¬å®ƒè‡ªå·±ã€‚

The source code for this whole compiler is [on GitHub](https://github.com/beezwax/markdown-compiler), feel free to clone it and see the parser files in your favorite editor as you read this, the snippets here are just for concept representation.
æ•´ä¸ªç¼–è¯‘å™¨çš„æºä»£ç åœ¨GitHubä¸Šï¼Œä¸‹é¢çš„æ ¸å¿ƒç‰‡æ®µåªæ˜¯ä¸ºäº†é˜è¿°æ¦‚å¿µ

Letâ€™s tart with the `TEXT` rule, which matches a single `TEXT` token.
å…ˆè¯´TEXTè§„åˆ™ï¼Œå®ƒåŒ¹é…ä¸€ä¸ªå•ä¸€çš„TEXTæ ‡è®°ã€‚

```ruby
class TextParser < BaseParser
  def match(tokens)
    return Node.null unless tokens.peek('TEXT')
    Node.new(type: 'TEXT', value: tokens.first.value, consumed: 1)
  end
end
```

All parsers behave very similarly. They take a list of `tokens` and peek the list to see if the sequence is correct. They then return the matched node, or an empty node (a null node). We call the result *node* because we want to build an abstract syntax tree.
æ‰€æœ‰è§£æå™¨çš„è¡Œä¸ºéƒ½å¾ˆç›¸ä¼¼ã€‚å®ƒä»¬æ¥å—ä¸€ä¸ªæ ‡è®°åˆ—è¡¨ï¼Œå¹¶peekè¯¥åˆ—è¡¨ï¼Œä»¥ç¡®å®šå…¶åºåˆ—æ˜¯å¦æ­£ç¡®ã€‚ç„¶åå®ƒä»¬ä¼šè¿”å›åŒ¹é…çš„èŠ‚ç‚¹ï¼Œæˆ–è€…ä¸€ä¸ªç©ºèŠ‚ç‚¹ï¼ˆNullNodeï¼‰ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸ºç»“æœèŠ‚ç‚¹ï¼Œå› ä¸ºæˆ‘ä»¬æƒ³å»ºç«‹ä¸€ä¸ªæŠ½è±¡çš„è¯­æ³•æ ‘ã€‚

Letâ€™s see a parser a bit more complicated, `__bold__ **text**`:
è®©æˆ‘ä»¬çœ‹çœ‹ä¸€ä¸ªæ›´å¤æ‚çš„è§£æå™¨ï¼Œ `__bold__ **text**`:

```ruby
class BoldParser < BaseParser
  def match(tokens)
    return Node.null unless tokens.peek_or(%w(UNDERSCORE UNDERSCORE TEXT UNDERSCORE UNDERSCORE), %w(STAR STAR TEXT STAR STAR))
    Node.new(type: 'BOLD', value: tokens.third.value, consumed: 5)
  end
end
```

Once again, we just check the token sequence is valid and return a node. The `peek_or` method lives in a `TokenList` object, it takes any amount of arrays as input and tries the tokens defined in each array one by one. It stops whenever it finds a match, returning true, otherwise it returns false. As you might imagine, the order of the arrays are very important, as itâ€™s first-in-first-matched.
å†ä¸€æ¬¡ï¼Œæˆ‘ä»¬åªæ˜¯æ£€æŸ¥tokenåºåˆ—æ˜¯å¦æœ‰æ•ˆå¹¶è¿”å›ä¸€ä¸ªèŠ‚ç‚¹ã€‚`peek_oræ˜¯`TokenList`å¯¹è±¡çš„æ–¹æ³•ï¼Œå®ƒæ¥å—ä»»ä½•æ•°é‡çš„æ•°ç»„ä½œä¸ºè¾“å…¥ï¼Œå¹¶é€ä¸€å°è¯•æ¯ä¸ªæ•°ç»„ä¸­å®šä¹‰çš„æ ‡è®°ã€‚åªè¦æ‰¾åˆ°ä¸€ä¸ªåŒ¹é…çš„ï¼Œå®ƒå°±åœæ­¢ï¼Œè¿”å›trueï¼Œå¦åˆ™å°±è¿”å›falseã€‚æ•°ç»„çš„é¡ºåºéå¸¸é‡è¦ï¼Œå› ä¸ºå®ƒæ˜¯å…ˆæ¥çš„å…ˆåŒ¹é…çš„ã€‚

The emphasis parser is quite similar to this one, so letâ€™s move onto something more interesting: The sentence parser. Our rule is `Sentence := EmphasizedText | BoldText | Text`. Seems simple enough, `match_first` does the trick fos us:
å¼ºè°ƒæ–‡å­—è§£æå™¨å’Œè¿™ä¸ªè§£æå™¨å¾ˆç›¸ä¼¼ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æ›´æœ‰è¶£çš„ä¸œè¥¿ï¼šå¥å­åˆ†æå™¨ã€‚æˆ‘ä»¬çš„è§„åˆ™æ˜¯ `Sentence := EmphasizedText | BoldText | Text`ã€‚çœ‹ä¸Šå»å¾ˆç®€å•ï¼Œ`match_first`æ–¹æ³•èƒ½å¸®æˆ‘ä»¬è§£å†³è¿™ä¸ªé—®é¢˜ï¼š

```ruby
class SentenceParser < BaseParser
  def match(tokens)
    match_first tokens, emphasis_parser, bold_parser, text_parser
  end
end
```

`match_first` is a concern which is included whenever needed. Itâ€™s somewhat like an *or*, it will try the given parsers and return the first valid node it finds. As usual, the order of the parsers is very important as they get tested in the given order.
`match_first`æ˜¯ä¸€ä¸ªconcernï¼Œåœ¨éœ€è¦çš„æ—¶å€™ä¼šè¢«åŒ…å«è¿›å»ã€‚å®ƒæœ‰ç‚¹åƒä¸€ä¸ªorï¼Œå®ƒå°†å°è¯•ç»™å®šçš„åˆ†æå™¨å¹¶è¿”å›å®ƒå‘ç°çš„ç¬¬ä¸€ä¸ªæœ‰æ•ˆèŠ‚ç‚¹ã€‚åƒå¾€å¸¸ä¸€æ ·ï¼Œåˆ†æå™¨çš„é¡ºåºæ˜¯éå¸¸é‡è¦çš„ï¼Œå› ä¸ºå®ƒä»¬ä¼šæŒ‰ç…§ç»™å®šçš„é¡ºåºè¿›è¡Œæµ‹è¯•ã€‚

Now, onto the next rule: `SentenceAndNewline := Sentence+ NEWLINE NEWLINE`.
ç°åœ¨ï¼Œè¿›å…¥ä¸‹ä¸€æ¡è§„åˆ™ï¼š `SentenceAndNewline := Sentence+ NEWLINE NEWLINE`

```ruby
class SentencesAndNewlineParser < BaseParser
  include MatchesStar

  def match(tokens)
    nodes, consumed = match_star tokens, with: sentence_parser
    return Node.null if nodes.empty?
    return Node.null unless tokens.peek_at(consumed, 'NEWLINE', 'NEWLINE')
    consumed += 2 # consume newlines

    ParagraphNode.new(sentences: nodes, consumed: consumed)
  end
end
```

Similar to `match_first`, we now have another concern, `match_star`, which matches something 0 or more times. Because we are matching a `+` (Kleen Plus), we want to match something once or more, so we error if we got nothing from our `match_star` method. Then we just match the two `NEWLINE` and return the node.
ä¸`match_first`ç±»ä¼¼ï¼Œæˆ‘ä»¬ç°åœ¨æœ‰å¦ä¸€ä¸ªconcernï¼Œå³`match_star`ï¼Œå®ƒå¯ä»¥åŒ¹é…0æ¬¡æˆ–æ›´å¤šæ¬¡ã€‚å› ä¸ºæˆ‘ä»¬æ­£åœ¨åŒ¹é…ä¸€ä¸ª`+`ï¼ˆKleen Plusï¼‰ï¼Œæˆ‘ä»¬æƒ³åŒ¹é…ä¸€æ¬¡æˆ–æ›´å¤šçš„ä¸œè¥¿ï¼Œæ‰€ä»¥å¦‚æœæˆ‘ä»¬çš„`match_star`æ–¹æ³•æ²¡æœ‰å¾—åˆ°ä»»ä½•ä¸œè¥¿ï¼Œæˆ‘ä»¬ä¼šå‡ºé”™ã€‚ç„¶åæˆ‘ä»¬åªæ˜¯åŒ¹é…ä¸¤ä¸ª`NEWLINE`å¹¶è¿”å›èŠ‚ç‚¹ã€‚

> **NOTE** We could make a new helper, `match_plus` here. To keep things simple, I decided to do it â€œmanuallyâ€. If you want to play around with the code, implementing `match_plus` is a good exercise.
> æ³¨æ„ æˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡Œåšä¸€ä¸ªæ–°çš„è¾…åŠ©å·¥å…·ï¼Œå³ match_plusã€‚ä¸ºäº†ä½¿äº‹æƒ…ç®€å•ï¼Œæˆ‘å†³å®š "æ‰‹åŠ¨ "å®Œæˆå®ƒã€‚å¦‚æœä½ æƒ³ç©ç©ä»£ç ï¼Œå®ç° match_plus æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ç»ƒä¹ ã€‚

Our little concerns take away most of the job, as the remaining parsers are quite trivial. For example, this is our `Body` parser:
æˆ‘ä»¬çš„å°concernså¸¦èµ°äº†å¤§éƒ¨åˆ†çš„å·¥ä½œï¼Œå› ä¸ºå‰©ä¸‹çš„è§£æå™¨æ˜¯éå¸¸å¾®ä¸è¶³é“çš„ã€‚ä¾‹å¦‚ï¼Œè¿™æ˜¯æˆ‘ä»¬çš„Bodyè§£æå™¨ã€‚

```ruby
class BodyParser < BaseParser
  include MatchesStar

  def match(tokens)
    nodes, consumed = match_star tokens, with: paragraph_parser
    return Node.null if nodes.empty?
    BodyNode.new(paragraphs: nodes, consumed: consumed)
  end
end
```

Now whatâ€™s missing is something to start calling up parsers. Letâ€™s wrap the whole parsing functionality into a `Parser` object which abstracts away all the complicated stuff into a simple API:
ç°åœ¨æ‰€ç¼ºå°‘çš„æ˜¯å¼€å§‹è°ƒç”¨è§£æå™¨çš„ä¸œè¥¿ã€‚è®©æˆ‘ä»¬æŠŠæ•´ä¸ªè§£æåŠŸèƒ½åŒ…è£…æˆä¸€ä¸ªè§£æå™¨å¯¹è±¡ï¼ŒæŠŠæ‰€æœ‰å¤æ‚çš„ä¸œè¥¿æŠ½è±¡æˆä¸€ä¸ªç®€å•çš„API

```ruby
class Parser
  def parse(tokens)
    body = body_parser.match(tokens)
    raise "Syntax error: #{tokens[body.consumed]}" unless tokens.count == body.consumed
    body
  end

  private

  def body_parser
    @body_parser ||= ParserFactory.build(:body_parser)
  end
end
```

Thatâ€™s it! å°±æ˜¯è¿™æ ·!

Weâ€™ve made it a long way so far. We can now transform `__Foo__ and *bar*.\n\nAnother paragraph.` into a tree data structure:
åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»åšäº†å¾ˆå¤šã€‚ç°åœ¨å¯ä»¥å°† `__Foo__ and *bar*.\n\nAnother paragraph.` è½¬åŒ–ä¸ºä¸€ä¸ªæ ‘å½¢æ•°æ®ç»“æ„ï¼š

```ruby
Parser.new.parse("__Foo__ and *bar*.\n\nAnother paragraph.") 
  => #<BodyNode:0x007fc774abe008
     @consumed=14,
     @paragraphs=
      [#<ParagraphNode:0x007fc774eb25d8
        @consumed=12,
        @sentences=
         [#<Node:0x007fc774ea8150 @consumed=5, @type="BOLD", @value="Foo">,
          #<Node:0x007fc774ac5f60 @consumed=1, @type="TEXT", @value=" and ">,
          #<Node:0x007fc774ac6758 @consumed=3, @type="EMPHASIS", @value="bar">,
          #<Node:0x007fc774eb33e8 @consumed=1, @type="TEXT", @value=".">]>,
       #<ParagraphNode:0x007fc774eb0828 @consumed=2, @sentences=[#<Node:0x007fc774eb1610 @consumed=1, @type="TEXT", @value="Another paragraph.">]>]>

```

That is no easy feat! Weâ€™ve talked a lot about parsers, grammars, tokens and all that stuff. More than enough to sip in for a day. Remember the source code is [on GitHub](https://github.com/beezwax/markdown-compiler), feel free to clone and play around with it.
è¿™å¯çœŸä¸å®¹æ˜“å•Šï¼æˆ‘ä»¬å·²ç»è°ˆäº†å¾ˆå¤šå…³äºè§£æå™¨ã€è¯­æ³•ã€æ ‡è®°ç­‰ä¸œè¥¿ï¼Œæºä»£ç åœ¨GitHubä¸Šã€‚
